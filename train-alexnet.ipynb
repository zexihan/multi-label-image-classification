{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import load_images, get_multi_hot_labels\n",
    "from alexnet_model import alexnet_model_fn\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_without_missing.csv')\n",
    "validation_df = pd.read_csv('data/validation.csv')\n",
    "\n",
    "train_path_list = train_df['imagePath']\n",
    "eval_path_list = validation_df['imagePath']\n",
    "\n",
    "eval_data = load_images(eval_path_list)\n",
    "eval_labels = get_multi_hot_labels(validation_df, list(range(validation_df.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data.shape, eval_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    train_iter_size = 5000\n",
    "    num_iters = 20\n",
    "    batch_size = 1\n",
    "    steps = 5000\n",
    "    eval_every_iters = 1\n",
    "    \n",
    "    #train_steps = []\n",
    "    #train_losses = []\n",
    "    \n",
    "    eval_steps = []\n",
    "    eval_losses = []\n",
    "    eval_precision = []\n",
    "    eval_recall = []\n",
    "    eval_meanfscore = []\n",
    "    \n",
    "    # Create the Estimator\n",
    "    multilabel_classifier = tf.estimator.Estimator(\n",
    "        model_fn=alexnet_model_fn, model_dir=\"model/multilabel_alexnet_model\")\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    #tensors_to_log = {\"probabilities\": \"sigmoid_tensor\"}\n",
    "    #tensors_to_log = {\"meanfscore\": \"eval_tensor\"}\n",
    "    tensors_to_log = []\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "        tensors=tensors_to_log, every_n_iter=100)\n",
    "    \n",
    "    for k in range(num_iters):\n",
    "        print('Trained images so far: {}'.format(k * train_iter_size))\n",
    "        \n",
    "        # Randomly load training data and labels\n",
    "        print('Loading train images..')\n",
    "        random_indices = np.random.randint(0, train_df.shape[0], size=train_iter_size)        \n",
    "        train_paths = [train_path_list[i] for i in random_indices]\n",
    "        train_data = load_images(train_paths)\n",
    "        \n",
    "        print('Loading train labels..')\n",
    "        train_labels = get_multi_hot_labels(train_df, random_indices)\n",
    "\n",
    "        # Train the model\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": train_data},\n",
    "            y=train_labels,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=None,\n",
    "            shuffle=True)\n",
    "        multilabel_classifier.train(\n",
    "            input_fn=train_input_fn,\n",
    "            steps=steps,\n",
    "            hooks=[logging_hook])\n",
    "        \n",
    "        if k % eval_every_iters == 0:\n",
    "            # Evaluate the model and print results\n",
    "            eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                x={\"x\": eval_data},\n",
    "                y=eval_labels,\n",
    "                shuffle=False)\n",
    "            eval_results = multilabel_classifier.evaluate(input_fn=eval_input_fn)\n",
    "            print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            print(eval_results)\n",
    "            \n",
    "            eval_steps.append(eval_results['global_step'])\n",
    "            eval_losses.append(eval_results['loss'])\n",
    "            eval_precision.append(eval_results['precision_micro'])\n",
    "            eval_recall.append(eval_results['recall_micro'])\n",
    "            eval_meanfscore.append(eval_results['meanfscore'])\n",
    "        \n",
    "        # Garbage collection\n",
    "        train_data = None\n",
    "        train_labels = None\n",
    "        gc.collect()\n",
    "    \n",
    "    eval_track = {'eval_steps':eval_steps, \n",
    "                  'eval_losses':eval_losses, \n",
    "                  'eval_precision':eval_precision, \n",
    "                  'eval_recall':eval_recall, \n",
    "                  'eval_meanfscore':eval_meanfscore}\n",
    "    \n",
    "    return eval_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_track = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(eval_track['eval_steps'], eval_track['eval_losses'])\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(eval_track['eval_steps'], eval_track['eval_meanfscore'])\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Validation meanfscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(eval_track).to_csv(\"eval_track_bs1_ep1.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "multilabel_classifier = tf.estimator.Estimator(\n",
    "            model_fn=alexnet_model_fn, model_dir=\"model\\\\multilabel_alexnet_model\")\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": eval_data},\n",
    "            y=eval_labels,\n",
    "            shuffle=False)\n",
    "eval_results = multilabel_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for validation set\n",
    "multilabel_classifier = tf.estimator.Estimator(\n",
    "            model_fn=alexnet_model_fn, model_dir=\"model\\\\multilabel_alexnet_model\")\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": eval_data},\n",
    "            y=eval_labels,\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "predictions = multilabel_classifier.predict(input_fn=eval_input_fn)\n",
    "y_predicted = np.array(list(p['classes'] for p in predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [np.where(row == 1) for row in y_predicted]\n",
    "predictions_df = pd.DataFrame({'predicted_labels': predicted_labels})\n",
    "predictions_df.style.set_properties(subset=['predicted_labels'], **{'width': '700px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [np.where(row == 1) for row in eval_labels]\n",
    "groundtruth_df = pd.DataFrame({'true_labels': true_labels})\n",
    "groundtruth_df.style.set_properties(subset=['true_labels'], **{'width': '700px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.style.set_properties(subset=['label_id'], **{'width': '700px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
